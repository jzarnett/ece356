\input{configuration}

\title{Lecture 32 --- Recovery: Repair, Probability }

\author{Jeff Zarnett \\ \small \texttt{jzarnett@uwaterloo.ca}}
\institute{Department of Electrical and Computer Engineering \\
  University of Waterloo}
\date{\today}


\begin{document}

\begin{frame}
  \titlepage

 \end{frame}

\begin{frame}
\frametitle{Probabilistically Answering Queries}

Residues and repairs are not the only way to return consistent answers. 

When there are several options, we can examine these options and make a determination of which is more likely. 

We form candidates - pretenders to the throne of the correct database -  by breaking up the possibilities for repair into all of their possible variants. 

 \end{frame}

\begin{frame}
\frametitle{Probabilistically Answering Queries}



In each candidate database, one tuple from each cluster is selected. 

Both candidate databases will receive a probability of being the ``correct" one.

\begin{center}
	\includegraphics[width=0.75\textwidth]{images/guessing.png}
\end{center}

\end{frame}


\begin{frame}
\frametitle{Remember Salaries?}

\begin{table}[h]\begin{center}
        \begin{tabular}{r | c  c} 
					salaries & employee\_name & salary \\ \hline
	           		 & J. Page  & 50 000 \\ 
	         		 & J. Page  & 80 000 \\ 
					 & V. Smith & 35 000 \\ 
					 & M. Stowe & 75 000 \\ 
        \end{tabular}
\end{center}\end{table}

 In the simplest solution, the database will just answer queries and attach the probability of the answer's correctness as another attribute of the tuple.

\begin{table}[h]\begin{center}
        \begin{tabular}{r | c  c  c} 
					salaries & employee\_name & salary & probability\\ \hline
	           		 & J. Page  & 50 000 & 0.1 \\ 
	         		 & J. Page  & 80 000 & 0.9 \\ 
					 & V. Smith & 35 000 & 0.4 \\ 
					 & M. Stowe & 75 000 & 1 \\ 
        \end{tabular}
\end{center}\end{table}


\end{frame}


\begin{frame}
\frametitle{Are We Sure?}

Suppose the query being asked were all the names of all employees making more than \$70~000. 

In the trivial case, M. Stowe's salary is certainly greater than \$70~000, since its probability is 1 (completely certain). 

\end{frame}


\begin{frame}
\frametitle{Are We Sure?}

Uncertainty enters the picture when examining the J. Page tuples. 

We note probability of his salary being \$80~000 is 0.9, so we include him in the return set and indicate the attached probability.

If the query were names of employees making more than \$45~000, then we would return J. Page with probability 1.

\end{frame}


\begin{frame}
\frametitle{Query Modification}

Or: Modify the requested queries to include the probability attributes. If the original query was:

\texttt{SELECT s.employee\_name FROM salaries s WHERE s.salary > 70~000}


Then the only changes are the addition of\texttt{SUM(s.probability)} and \texttt{GROUP BY s.employee\_name}, so that the rebuilt query reads:


\texttt{SELECT s.employee\_name, SUM(s.probability) FROM salaries s WHERE s.salary $>$ 70~000 GROUP BY s.employee\_name}


\end{frame}


\begin{frame}
\frametitle{The Possibilities Multiply}

In a more complex query, we simply multiply the probabilities. 

\texttt{SELECT o.id, c.id FROM order o, customer c  WHERE o.cIdFk = c.id AND c.balance > 10000 }


This query is modified in the same way as that of the preceding paragraph, except the sum statement reads \texttt{SUM(o.probability * c.probability)}.

The group statement is \texttt{GROUP BY o.id, c.id}.


\end{frame}


\begin{frame}
\frametitle{Determining Probabilities}

The most pressing question is how we determine the probabilities. 

\begin{figure}[!h]
  \centering \includegraphics[width=0.6\textwidth]{images/formula.pdf}
\end{figure}

\end{frame}


\begin{frame}
\frametitle{Example: Inconsistent Customers}

\begin{table}[h]\begin{center}
        \begin{tabular}{r | c  c  c  c } 
					customers & name & market\_segment & country & address\\ \hline
	           		 & Mary   & building & USA    & 123 Jones Ave. \\ 
	         		 & Mary   & banking  & Canada & 123 Jones Ave. \\ 
					 & Marion & banking  & USA    & 123 Jones Ave. \\ 
        \end{tabular}
        
\end{center}\end{table}

The most common values in the database are probably the correct ones.

\end{frame}


\begin{frame}
\frametitle{Inconsistent Data Probability}

The algorithm shown above follows this same intuition. 

The tuple we will consider correct is the one closest to the representative. 

For numerical data, similarity between two figures can be computed. 

\end{frame}


\begin{frame}
\frametitle{Inconsistent Data Probability}

For data for which there is no obvious distance measure, we term them \alert{categorical data}; information loss as the distance metric.

Information loss is simply a measure of the difference between  the tuple in question and the representative. 

\end{frame}

\begin{frame}
\frametitle{Probabilistically  Imperfect}

The strategy presented is imperfect, however, since we might fail to produce clean answers for some classes of query.

\texttt{select c.id from order o, customer c  where o.quantity < 5 and o.cIdFk = c.id and c.balance > 25~000}. 

This fails because the join between \texttt{c} and \texttt{o} incorrectly double-counts some probabilities.

\end{frame}


\begin{frame}
\frametitle{Choosing the Representative Data?}

Open question: how we might find a representative sample when it is not obviously in a majority-rules scenario.

Whichever is picked to be the representative will not differ from the representative (by definition), so it will receive a probability of 1. 

\begin{center}
	\includegraphics[width=0.4\textwidth]{images/hesme.jpg}
\end{center}

\end{frame}


\begin{frame}
\frametitle{Choosing the Representative Data?}

Thus, whatever we (perhaps randomly) choose to be our representative is the eventual winner and will be considered correct. 

It is clear that we need to come to some decision, but reporting 100\% certainty about data which is, at best, 50\% certain is misleading. 

\end{frame}

\begin{frame}
\frametitle{Popular Answer...}

This analysis also equates popular with correct.

If an incorrect answer appears in the database twice and a correct answer once, then the incorrect answer will be deemed correct, since it is more popular. 

\begin{center}
	\includegraphics[width=0.5\textwidth]{images/vote.jpg}
\end{center}



\end{frame}

\begin{frame}
\frametitle{Popular Answer...}

That aside, there is not much that can be done about this problem... 

Even a human observing the database might be more likely to conclude that the popular answer is the correct one. 

Unless they had some other external knowledge?

\end{frame}


\begin{frame}
\frametitle{Computational Complexity of Repair \& Probability}

Computational complexity is broken down into a short analysis on each of the methods detailed previously.

Computational complexity may disqualify a method from being practically useful, should it take too much time to reach a reasonable answer.

\begin{center}
	\includegraphics[width=0.35\textwidth]{images/pleasewait.jpg}
\end{center}


\end{frame}


\begin{frame}
\frametitle{Complexity of Repairs}

Assume that we are looking at repairs that are subsets of the original database.

Then repair checking is in polynomial time for arbitrary constraints combined with acyclic dependencies. 

Should any of these constraints not hold, the problem is pushed into the realm of co-NP-complete problems.


\end{frame}

\begin{frame}
\frametitle{Complexity of Query Transformation}

The process of query transformation is shown to have a polynomial time computability of result tuples. 

The transformed query will be first order as long as the original query is as well. 

The query transformation does not require examining all the possible repairs.

We can evaluate a query with an exponential number of possible repairs in polynomial time.

\end{frame}


\begin{frame}
\frametitle{Aggregate Query Transformation}

Aggregate queries: build a conflict graph; a standard graph with nodes/edges. 

In the graph, maximal independent sets - that is, sets that are the farthest apart in terms of data equality - represent possible repairs of the database. 

As long as there is at most one nontrivial constraint, all operators except \texttt{COUNT} are polynomial time.

\end{frame}


\begin{frame}
\frametitle{Aggregate Query Transformation}

If there are many nontrivial constraints, then the problem of finding lower and upper bounds becomes NP-complete. 

The \texttt{COUNT} operation is always NP-complete.


\end{frame}


\begin{frame}
\frametitle{Probabilistic Complexity}

\begin{figure}[!h]
  \centering \includegraphics[width=0.8\textwidth]{images/CAPerformance.pdf}
\end{figure}


\end{frame}


\begin{frame}
\frametitle{Future Work}

To get completeness for disjunctive and existential queries, they will need to move beyond simple query transformations.

Database repairs examined are based on differences against the whole tuple and does not permit modifying attributes of tuples.

The notes cover various deficiencies in the papers that went into this topic...

\end{frame}


\begin{frame}
\frametitle{Conclusion}

There is no single right answer when dealing with an inconsistent database. 

However inconsistent it may be, it is unlikely that we will be able to unequivocally say what is correct and what is not. 

\end{frame}




\begin{frame}
\frametitle{Conclusion}

\begin{center}
	\includegraphics[width=0.5\textwidth]{images/soupdujour.jpg}
\end{center}
\end{frame}




\begin{frame}
\frametitle{Conclusion}

With these techniques, we can get back consistent answers to our queries. 



\begin{center}
	\includegraphics[width=0.4\textwidth]{images/close-enough.jpg}
\end{center}

The success of aggregate queries is less certain; chances are still reasonable.

\end{frame}






\end{document}

